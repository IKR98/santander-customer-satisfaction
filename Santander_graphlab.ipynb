{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab as gl\n",
    "\n",
    "#needed for charts\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/ellfae/Dropbox/MyResearch/kaggle/Santander customer satisfaction/train.csv/train.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/ellfae/Dropbox/MyResearch/kaggle/Santander customer satisfaction/train.csv/train.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 4.14184 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 4.14184 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[int,int,int,int,float,float,float,float,int,int,float,float,float,int,int,float,int,int,float,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,float,float,int,float,float,int,int,int,int,int,float,int,float,float,float,float,int,int,int,float,float,int,int,int,float,float,int,float,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,float,float,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,float,float,float,float,float,float,float,float,float,int,float,float,float,float,int,float,int,int,int,int,int,int,int,int,float,int,float,float,int,int,int,int,int,int,int,int,int,int,int,int,float,int]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 67150 lines. Lines per second: 8622.37</pre>"
      ],
      "text/plain": [
       "Read 67150 lines. Lines per second: 8622.37"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/ellfae/Dropbox/MyResearch/kaggle/Santander customer satisfaction/train.csv/train.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/ellfae/Dropbox/MyResearch/kaggle/Santander customer satisfaction/train.csv/train.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 76020 lines in 8.15498 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 76020 lines in 8.15498 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/ellfae/Dropbox/MyResearch/kaggle/Santander customer satisfaction/test.csv/test.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/ellfae/Dropbox/MyResearch/kaggle/Santander customer satisfaction/test.csv/test.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 2.26467 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 2.26467 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[int,int,int,int,float,float,int,int,int,int,int,float,float,float,float,float,float,float,float,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,float,int,float,float,float,int,int,float,int,int,int,int,float,float,float,int,int,int,float,float,float,float,int,float,int,int,float,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,float,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,float,float,float,float,int,float,float,float,float,float,float,float,float,float,float,float,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,int,float,int,int,float,int,int,int,int,float]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 67325 lines. Lines per second: 13540.7</pre>"
      ],
      "text/plain": [
       "Read 67325 lines. Lines per second: 13540.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/ellfae/Dropbox/MyResearch/kaggle/Santander customer satisfaction/test.csv/test.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/ellfae/Dropbox/MyResearch/kaggle/Santander customer satisfaction/test.csv/test.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 75818 lines in 5.43044 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 75818 lines in 5.43044 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading the train and test data\n",
    "customers = gl.SFrame('train.csv/train.csv')\n",
    "test_sframe = gl.SFrame('test.csv/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gl.canvas.set_target('ipynb')\n",
    "#customers['TARGET'].show(view='Categorical')\n",
    "#customers.head().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371\n"
     ]
    }
   ],
   "source": [
    "print customers.num_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing constant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove all columns with constant columns; where STD == 0\n",
    "remove = []\n",
    "\n",
    "for col in customers.column_names():\n",
    "    if customers[col].std() == 0:\n",
    "        remove.append(col)\n",
    "\n",
    "customers.remove_columns(remove)\n",
    "customers.num_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prediction from sklearn model\n",
    "import pickle\n",
    "data = pickle.load(open(\"my_features.pickle\", \"rb\"))\n",
    "sums = pickle.load(open(\"sums.pickle\", \"rb\"))\n",
    "customers['sum_of_0'] = sums['sum_of_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing duplicated columns (too slow!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309\n"
     ]
    }
   ],
   "source": [
    "#computed on kaggle\n",
    "remove_duplicated = ['ind_var29_0', 'ind_var29', 'ind_var13_medio', 'ind_var18', 'ind_var26', \n",
    "                     'ind_var25', 'ind_var32', 'ind_var34', 'ind_var37', 'ind_var39', 'num_var29_0', \n",
    "                     'num_var29', 'num_var13_medio', 'num_var18', 'num_var26', 'num_var25', 'num_var32', \n",
    "                     'num_var34', 'num_var37', 'num_var39', 'saldo_var29', 'saldo_medio_var13_medio_ult1', \n",
    "                     'delta_num_reemb_var13_1y3', 'delta_num_reemb_var17_1y3', 'delta_num_reemb_var33_1y3', \n",
    "                     'delta_num_trasp_var17_in_1y3', 'delta_num_trasp_var17_out_1y3', \n",
    "                     'delta_num_trasp_var33_in_1y3', 'delta_num_trasp_var33_out_1y3']\n",
    "\n",
    "customers.remove_columns(remove_duplicated)\n",
    "print customers.num_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove low variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it did improve the results a little -> 0.811884 -> 0.812585\n",
    "# These features were spitted with low variance from the classifier\n",
    "low_variance_features = ['delta_imp_reemb_var33_1y3',  'imp_reemb_var17_hace3', 'imp_reemb_var33_ult1', 'imp_trasp_var17_in_hace3', 'num_reemb_var17_hace3', 'num_reemb_var33_ult1', 'num_trasp_var17_in_hace3']\n",
    "customers.remove_columns(low_variance_features)\n",
    "customers.num_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#customers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Target are 0, which means most customers are satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = customers.random_split(.8, seed=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Boosted Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Boosted trees classifier:</pre>"
      ],
      "text/plain": [
       "Boosted trees classifier:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 57865</pre>"
      ],
      "text/plain": [
       "Number of examples          : 57865"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of classes           : 2</pre>"
      ],
      "text/plain": [
       "Number of classes           : 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of feature columns   : 301</pre>"
      ],
      "text/plain": [
       "Number of feature columns   : 301"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 301</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 301"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+--------------+--------------+----------------+</pre>"
      ],
      "text/plain": [
       "+-----------+--------------+--------------+----------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Elapsed Time | Training-auc | Validation-auc |</pre>"
      ],
      "text/plain": [
       "| Iteration | Elapsed Time | Training-auc | Validation-auc |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+--------------+--------------+----------------+</pre>"
      ],
      "text/plain": [
       "+-----------+--------------+--------------+----------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 0.569986     | 0.809100     | 0.799920       |</pre>"
      ],
      "text/plain": [
       "| 1         | 0.569986     | 0.809100     | 0.799920       |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 2         | 1.231362     | 0.813318     | 0.807234       |</pre>"
      ],
      "text/plain": [
       "| 2         | 1.231362     | 0.813318     | 0.807234       |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 3         | 1.903624     | 0.832645     | 0.823401       |</pre>"
      ],
      "text/plain": [
       "| 3         | 1.903624     | 0.832645     | 0.823401       |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 4         | 2.492083     | 0.839011     | 0.828281       |</pre>"
      ],
      "text/plain": [
       "| 4         | 2.492083     | 0.839011     | 0.828281       |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 5         | 3.152016     | 0.837823     | 0.825267       |</pre>"
      ],
      "text/plain": [
       "| 5         | 3.152016     | 0.837823     | 0.825267       |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 6         | 3.699568     | 0.843437     | 0.832357       |</pre>"
      ],
      "text/plain": [
       "| 6         | 3.699568     | 0.843437     | 0.832357       |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 10        | 5.413289     | 0.856914     | 0.839825       |</pre>"
      ],
      "text/plain": [
       "| 10        | 5.413289     | 0.856914     | 0.839825       |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+--------------+--------------+----------------+</pre>"
      ],
      "text/plain": [
       "+-----------+--------------+--------------+----------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "customer_model = gl.boosted_trees_classifier.create(train_data, target='TARGET', row_subsample=0.7, \n",
    "                                                    max_depth=5, metric='auc',\n",
    "                                                   column_subsample=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Deep Learning Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.961961503208066,\n",
       " 'auc': 0.8303664113297732,\n",
       " 'confusion_matrix': Columns:\n",
       " \ttarget_label\tint\n",
       " \tpredicted_label\tint\n",
       " \tcount\tint\n",
       " \n",
       " Rows: 4\n",
       " \n",
       " Data:\n",
       " +--------------+-----------------+-------+\n",
       " | target_label | predicted_label | count |\n",
       " +--------------+-----------------+-------+\n",
       " |      0       |        1        |   5   |\n",
       " |      1       |        0        |  576  |\n",
       " |      0       |        0        | 14692 |\n",
       " |      1       |        1        |   1   |\n",
       " +--------------+-----------------+-------+\n",
       " [4 rows x 3 columns],\n",
       " 'f1_score': 0.003430531732418525,\n",
       " 'log_loss': 0.14047113436676062,\n",
       " 'precision': 0.16666666666666666,\n",
       " 'recall': 0.0017331022530329288,\n",
       " 'roc_curve': Columns:\n",
       " \tthreshold\tfloat\n",
       " \tfpr\tfloat\n",
       " \ttpr\tfloat\n",
       " \tp\tint\n",
       " \tn\tint\n",
       " \n",
       " Rows: 100001\n",
       " \n",
       " Data:\n",
       " +-----------+-----+-----+-----+-------+\n",
       " | threshold | fpr | tpr |  p  |   n   |\n",
       " +-----------+-----+-----+-----+-------+\n",
       " |    0.0    | 1.0 | 1.0 | 577 | 14697 |\n",
       " |   1e-05   | 1.0 | 1.0 | 577 | 14697 |\n",
       " |   2e-05   | 1.0 | 1.0 | 577 | 14697 |\n",
       " |   3e-05   | 1.0 | 1.0 | 577 | 14697 |\n",
       " |   4e-05   | 1.0 | 1.0 | 577 | 14697 |\n",
       " |   5e-05   | 1.0 | 1.0 | 577 | 14697 |\n",
       " |   6e-05   | 1.0 | 1.0 | 577 | 14697 |\n",
       " |   7e-05   | 1.0 | 1.0 | 577 | 14697 |\n",
       " |   8e-05   | 1.0 | 1.0 | 577 | 14697 |\n",
       " |   9e-05   | 1.0 | 1.0 | 577 | 14697 |\n",
       " +-----------+-----+-----+-----+-------+\n",
       " [100001 rows x 5 columns]\n",
       " Note: Only the head of the SFrame is printed.\n",
       " You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_1 = customer_model.predict(test_sframe, output_type='probability')\n",
    "prediction_2 = data['prediction_model_sklearn']\n",
    "prediction = [((a+b)/2) for a,b in zip(prediction_1, prediction_2)]\n",
    "prediction= gl.SArray(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function for making submission\n",
    "def make_submission(prediction, filename='submission.txt'):\n",
    "    with open(filename, 'w') as f:\n",
    "        #header\n",
    "        f.write('ID,TARGET\\n')\n",
    "        submission_strings = test_sframe['ID'].astype(str) + ',' + prediction.astype(str)\n",
    "        for row in submission_strings:\n",
    "            f.write(row + '\\n')\n",
    "\n",
    "make_submission(prediction, 'submission18.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# removing columns that are giving the same information\\nimport numpy as np\\nremove_duplicates = []\\nc = customers.column_names()\\n#test = customers[\\'TARGET\\'].to_numpy()\\nfor i in range(len(c) - 1):\\n    v = customers[c[i]].to_numpy()\\n    for j in range(i+1, len(c)):\\n        if np.array_equal(v, customers[c[j]].to_numpy()):\\n            remove_duplicates.append(c[j])\\n    print \"round\", i, \"done\"\\nprint remove_duplicates\\n#giving the same information ???\\n\\n\\n######################################################################3\\n\\ndef mortgage_divide(mortgage):\\n    if mortgage < 100000:\\n        return 1\\n    else:\\n        return 0\\ncustomers[\\'mortgage_divider\\'] = customers[\\'var38\\'].apply(mortgage_divide)\\n\\n#########################################################################\\n\\n\\n# at first only age = -999999 was considered but then we also replaced 0 with 2 and this also helped to improve result\\ndef fix_age(nat):\\n    if nat == -999999 or nat == 0:\\n        return 2\\n    else:\\n        return nat\\n\\ncustomers[\\'var3\\'] = customers[\\'var3\\'].apply(fix_age)\\n\\n==========================================================================\\n# select on the features, leaving out the \\'TARGET\\' feature\\nfeatures = [f for f in customers.column_names() if f != \\'TARGET\\' and f != \\'ID\\']\\n\\n## NOTE: Causes overfitting (need to study why)\\n# add the sum of 0\\'s\\ncustomers[\\'sum_of_0\\'] = customers[features].apply(lambda x: len([i for i in x.values() if i == 0]))\\n\\n============================================================================\\n\\n\\n#net = gl.deeplearning.create(train_data, target=\\'TARGET\\')\\nnet = gl.deeplearning.MultiLayerPerceptrons(num_hidden_layers=4,\\n                                           num_hidden_units=[32,64,32,2],\\n                                           activation=\\'relu\\',\\n                                           learning_rate=0.001,\\n                                           max_iterations=5000)\\n                                           \\ndl_customer_model = gl.neuralnet_classifier.create(train_data,\\'TARGET\\',\\n                                                  network=net,\\n                                                  valiation_data=test_data)\\n\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# removing columns that are giving the same information\n",
    "import numpy as np\n",
    "remove_duplicates = []\n",
    "c = customers.column_names()\n",
    "#test = customers['TARGET'].to_numpy()\n",
    "for i in range(len(c) - 1):\n",
    "    v = customers[c[i]].to_numpy()\n",
    "    for j in range(i+1, len(c)):\n",
    "        if np.array_equal(v, customers[c[j]].to_numpy()):\n",
    "            remove_duplicates.append(c[j])\n",
    "    print \"round\", i, \"done\"\n",
    "print remove_duplicates\n",
    "#giving the same information ???\n",
    "\n",
    "\n",
    "######################################################################3\n",
    "\n",
    "def mortgage_divide(mortgage):\n",
    "    if mortgage < 100000:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "customers['mortgage_divider'] = customers['var38'].apply(mortgage_divide)\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "\n",
    "# at first only age = -999999 was considered but then we also replaced 0 with 2 and this also helped to improve result\n",
    "def fix_age(nat):\n",
    "    if nat == -999999 or nat == 0:\n",
    "        return 2\n",
    "    else:\n",
    "        return nat\n",
    "\n",
    "customers['var3'] = customers['var3'].apply(fix_age)\n",
    "\n",
    "==========================================================================\n",
    "# select on the features, leaving out the 'TARGET' feature\n",
    "features = [f for f in customers.column_names() if f != 'TARGET' and f != 'ID']\n",
    "\n",
    "## NOTE: Causes overfitting (need to study why)\n",
    "# add the sum of 0's\n",
    "customers['sum_of_0'] = customers[features].apply(lambda x: len([i for i in x.values() if i == 0]))\n",
    "\n",
    "============================================================================\n",
    "\n",
    "\n",
    "#net = gl.deeplearning.create(train_data, target='TARGET')\n",
    "net = gl.deeplearning.MultiLayerPerceptrons(num_hidden_layers=4,\n",
    "                                           num_hidden_units=[32,64,32,2],\n",
    "                                           activation='relu',\n",
    "                                           learning_rate=0.001,\n",
    "                                           max_iterations=5000)\n",
    "                                           \n",
    "dl_customer_model = gl.neuralnet_classifier.create(train_data,'TARGET',\n",
    "                                                  network=net,\n",
    "                                                  valiation_data=test_data)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
