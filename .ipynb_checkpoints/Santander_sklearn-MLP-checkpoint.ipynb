{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name MLPClassifier",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-20e0c0a48b58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOneClassSVM\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m \u001b[1;31m#Scaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name MLPClassifier"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import OneClassSVM\n",
    "import pickle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler #Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "df_train = pd.read_csv('train.csv/train.csv')\n",
    "df_test = pd.read_csv('test.csv/test.csv')\n",
    "\n",
    "#load the pickle where extra computer features are stored\n",
    "my_features = pickle.load(open(\"sums.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_to_clean = ['ind_var2_0', 'ind_var2', 'ind_var27_0', 'ind_var28_0', 'ind_var28', 'ind_var27', 'ind_var41', 'ind_var46_0', 'ind_var46', 'num_var27_0', 'num_var28_0', 'num_var28', 'num_var27', 'num_var41', 'num_var46_0', 'num_var46', 'saldo_var28', 'saldo_var27', 'saldo_var41', 'saldo_var46', 'imp_amort_var18_hace3', 'imp_amort_var34_hace3', 'imp_reemb_var13_hace3', 'imp_reemb_var33_hace3', 'imp_trasp_var17_out_hace3', 'imp_trasp_var33_out_hace3', 'num_var2_0_ult1', 'num_var2_ult1', 'num_reemb_var13_hace3', 'num_reemb_var33_hace3', 'num_trasp_var17_out_hace3', 'num_trasp_var33_out_hace3', 'saldo_var2_ult1', 'saldo_medio_var13_medio_hace3',\n",
    "                   'ind_var29_0', 'ind_var29', 'ind_var13_medio', 'ind_var18', 'ind_var26', \n",
    "                     'ind_var25', 'ind_var32', 'ind_var34', 'ind_var37', 'ind_var39', 'num_var29_0', \n",
    "                     'num_var29', 'num_var13_medio', 'num_var18', 'num_var26', 'num_var25', 'num_var32', \n",
    "                     'num_var34', 'num_var37', 'num_var39', 'saldo_var29', 'saldo_medio_var13_medio_ult1', \n",
    "                     'delta_num_reemb_var13_1y3', 'delta_num_reemb_var17_1y3', 'delta_num_reemb_var33_1y3', \n",
    "                     'delta_num_trasp_var17_in_1y3', 'delta_num_trasp_var17_out_1y3', \n",
    "                     'delta_num_trasp_var33_in_1y3', 'delta_num_trasp_var33_out_1y3',\n",
    "                   'delta_imp_reemb_var33_1y3',  'imp_reemb_var17_hace3', 'imp_reemb_var33_ult1', 'imp_trasp_var17_in_hace3', 'num_reemb_var17_hace3', 'num_reemb_var33_ult1', 'num_trasp_var17_in_hace3']\n",
    "\n",
    "\n",
    "\n",
    "df_train.drop(feature_to_clean, axis=1, inplace=True)\n",
    "df_test.drop(feature_to_clean, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add binary features for each value\n",
    "1. here I want to add additional columns for each value in each feature.\n",
    "2. For now I will just try add add \"1\" for where \"0\" is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_extraction = ['num_var13_corto', 'num_var13_corto_0', 'num_meses_var12_ult3', 'num_meses_var13_corto_ult3',\n",
    "                      'num_meses_var39_vig_ult3', 'num_meses_var5_ult3','num_var24_0','num_var12','var36',\n",
    "                      'num_var5','num_var5_0','num_var12_0','num_var13','num_var13_0','num_var42','num_var4',\n",
    "                      'num_var42_0','num_var30','num_var39_0','num_var41_0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add a binary new feature for each category in each of the features extraction\n",
    "'''\n",
    "def add_binary_features(cols,cat):\n",
    "    for value in cols:\n",
    "        if value == cat:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0 \n",
    "\n",
    "tracker = 1\n",
    "for f in features_extraction:\n",
    "    categories = np.intersect1d( df_test[f] , df_train[f])\n",
    "    #train_categories = df_train[f].unique()\n",
    "    for cat in categories:\n",
    "        df_train[f+\"_\"+str(cat)] = df_train[[f]].apply(add_binary_features, args=(cat,), axis=1)\n",
    "        df_test[f+\"_\"+str(cat)] = df_test[[f]].apply(add_binary_features, args=(cat,), axis=1)\n",
    "    \n",
    "    print \"Done with \" + str(tracker)\n",
    "    \n",
    "    tracker+=1\n",
    "    \n",
    "# TODO: drop the features\n",
    "df_train.drop(features_extraction, axis=1, inplace=True)\n",
    "df_test.drop(features_extraction, axis=1, inplace=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Sum of 0's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = [f for f in df_train.columns if f != 'TARGET' and f!='ID' ]\n",
    "#df_train['sum_of_0'] = my_features['sum_of_0']\n",
    "def calculate_zeros(cols):\n",
    "    sum_array = [] \n",
    "    for value in cols:\n",
    "        if value == 0:\n",
    "            sum_array.append(value)\n",
    "    return len(sum_array)\n",
    "\n",
    "df_train['sum_of_0'] = df_train[features].apply(calculate_zeros, axis=1)\n",
    "df_test['sum_of_0'] = df_test[features].apply(calculate_zeros, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare data for model\n",
    "y_train = df_train['TARGET'].values\n",
    "X_train = df_train.drop(['ID','TARGET'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_test = df_test['ID']\n",
    "X_test = df_test.drop(['ID'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_train = len(X_train)\n",
    "len_test = len(X_test)\n",
    "\n",
    "#scale data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# apply same transformation to test data\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(32,64,32), activation='relu', \n",
    "     beta_1=0.9, beta_2=0.999,\n",
    "     learning_rate_init=0.001, max_iter = 5000, random_state = 1235, \n",
    "     learning_rate='adaptive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fitting\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "y_pred= clf.predict_proba(X_test)[:,1]\n",
    "print('Overall AUC:', roc_auc_score(y_train, clf.predict_proba(X_train)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"ID\":id_test, \"TARGET\":y_pred})\n",
    "submission.to_csv(\"submission60.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ADDITIONAL FUNCTIONS FOR FEATURE ENGINEERING\n",
    "==============================================================\n",
    "import pickle\n",
    "to_save = {\n",
    "    \"sum_of_0\":df_train['sum_of_0']\n",
    "}\n",
    "pickle.dump(to_save, open('my_features.pickle','a'))\n",
    "===============================================================\n",
    "#NAME: ADD SUM OF 0'S\n",
    "# select on the features, leaving out the 'TARGET' feature\n",
    "\n",
    "\n",
    "features = [f for f in df_train.columns if f != 'TARGET' and f!='ID' ]\n",
    "\n",
    "def calculate_zeros(cols):\n",
    "    sum_array = [] \n",
    "    for value in cols:\n",
    "        if value == 0:\n",
    "            sum_array.append(value)\n",
    "    return len(sum_array)\n",
    "\n",
    "df_train['sum_of_0'] = df_train[features].apply(calculate_zeros, axis=1)\n",
    "\n",
    "===============================================================\n",
    "\n",
    "# remove duplicated columns\n",
    "remove = []\n",
    "c = df_train.columns\n",
    "for i in range(len(c)-1):\n",
    "    v = df_train[c[i]].values\n",
    "    for j in range(i+1, len(c)):\n",
    "        if np.array_equal(v, df_train[c[j]].values):\n",
    "            remove.append(c[j])\n",
    "\n",
    "df_train.drop(remove, axis=1, inplace=True)\n",
    "df_test.drop(remove, axis=1, inplace=True)\n",
    "\n",
    "================================================================\n",
    "\n",
    "# remove constant columns\n",
    "remove = []\n",
    "for col in df_train.columns:\n",
    "    if df_train[col].std() == 0:\n",
    "        remove.append(col)\n",
    "================================================================\n",
    "#fix nationality\n",
    "\n",
    "def fix_nationality(nat):\n",
    "    for value in nat:\n",
    "        if value == -999999:\n",
    "            return 2\n",
    "        else:\n",
    "            return value\n",
    "\n",
    "df_train['var3'] = df_train[['var3']].apply(fix_nationality, axis=1)\n",
    "df_test['var3'] = df_test[['var3']].apply(fix_nationality, axis=1)\n",
    "\n",
    "\n",
    "=================================================================\n",
    "# Add feature selection\n",
    "\n",
    "#Extract 49 features from all the features\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "clf_0 = ExtraTreesClassifier(random_state=1729)\n",
    "selector = clf_0.fit(X_train, y_train)\n",
    "\n",
    "fs = SelectFromModel(selector, prefit=True)\n",
    "\n",
    "X_train = fs.transform(X_train)\n",
    "X_eval = fs.transform(X_eval)\n",
    "X_test = fs.transform(X_test)\n",
    "=================================================\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
