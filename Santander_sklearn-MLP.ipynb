{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18.dev0\n",
      "['/home/ellfae/anaconda3/envs/py35/lib/python3.5/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import OneClassSVM\n",
    "import pickle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler #Scaler\n",
    "\n",
    "print (sklearn.__version__)\n",
    "import site\n",
    "print (site.getsitepackages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "df_train = pd.read_csv('train.csv/train.csv')\n",
    "df_test = pd.read_csv('test.csv/test.csv')\n",
    "\n",
    "#load the pickle where extra computer features are stored\n",
    "#my_features = pickle.load(open(\"sums.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_to_clean = ['ind_var2_0', 'ind_var2', 'ind_var27_0', 'ind_var28_0', 'ind_var28', 'ind_var27', 'ind_var41', 'ind_var46_0', 'ind_var46', 'num_var27_0', 'num_var28_0', 'num_var28', 'num_var27', 'num_var41', 'num_var46_0', 'num_var46', 'saldo_var28', 'saldo_var27', 'saldo_var41', 'saldo_var46', 'imp_amort_var18_hace3', 'imp_amort_var34_hace3', 'imp_reemb_var13_hace3', 'imp_reemb_var33_hace3', 'imp_trasp_var17_out_hace3', 'imp_trasp_var33_out_hace3', 'num_var2_0_ult1', 'num_var2_ult1', 'num_reemb_var13_hace3', 'num_reemb_var33_hace3', 'num_trasp_var17_out_hace3', 'num_trasp_var33_out_hace3', 'saldo_var2_ult1', 'saldo_medio_var13_medio_hace3',\n",
    "                   'ind_var29_0', 'ind_var29', 'ind_var13_medio', 'ind_var18', 'ind_var26', \n",
    "                     'ind_var25', 'ind_var32', 'ind_var34', 'ind_var37', 'ind_var39', 'num_var29_0', \n",
    "                     'num_var29', 'num_var13_medio', 'num_var18', 'num_var26', 'num_var25', 'num_var32', \n",
    "                     'num_var34', 'num_var37', 'num_var39', 'saldo_var29', 'saldo_medio_var13_medio_ult1', \n",
    "                     'delta_num_reemb_var13_1y3', 'delta_num_reemb_var17_1y3', 'delta_num_reemb_var33_1y3', \n",
    "                     'delta_num_trasp_var17_in_1y3', 'delta_num_trasp_var17_out_1y3', \n",
    "                     'delta_num_trasp_var33_in_1y3', 'delta_num_trasp_var33_out_1y3',\n",
    "                   'delta_imp_reemb_var33_1y3',  'imp_reemb_var17_hace3', 'imp_reemb_var33_ult1', 'imp_trasp_var17_in_hace3', 'num_reemb_var17_hace3', 'num_reemb_var33_ult1', 'num_trasp_var17_in_hace3']\n",
    "\n",
    "\n",
    "\n",
    "df_train.drop(feature_to_clean, axis=1, inplace=True)\n",
    "df_test.drop(feature_to_clean, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add binary features for each value\n",
    "1. here I want to add additional columns for each value in each feature.\n",
    "2. For now I will just try add add \"1\" for where \"0\" is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_extraction = ['num_var13_corto', 'num_var13_corto_0', 'num_meses_var12_ult3', 'num_meses_var13_corto_ult3',\n",
    "                      'num_meses_var39_vig_ult3', 'num_meses_var5_ult3','num_var24_0','num_var12','var36',\n",
    "                      'num_var5','num_var5_0','num_var12_0','num_var13','num_var13_0','num_var42','num_var4',\n",
    "                      'num_var42_0','num_var30','num_var39_0','num_var41_0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef add_binary_features(cols,cat):\\n    for value in cols:\\n        if value == cat:\\n            return 1\\n        else:\\n            return 0 \\n\\ntracker = 1\\nfor f in features_extraction:\\n    categories = np.intersect1d( df_test[f] , df_train[f])\\n    #train_categories = df_train[f].unique()\\n    for cat in categories:\\n        df_train[f+\"_\"+str(cat)] = df_train[[f]].apply(add_binary_features, args=(cat,), axis=1)\\n        df_test[f+\"_\"+str(cat)] = df_test[[f]].apply(add_binary_features, args=(cat,), axis=1)\\n    \\n    print \"Done with \" + str(tracker)\\n    \\n    tracker+=1\\n    \\n# TODO: drop the features\\ndf_train.drop(features_extraction, axis=1, inplace=True)\\ndf_test.drop(features_extraction, axis=1, inplace=True)\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add a binary new feature for each category in each of the features extraction\n",
    "'''\n",
    "def add_binary_features(cols,cat):\n",
    "    for value in cols:\n",
    "        if value == cat:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0 \n",
    "\n",
    "tracker = 1\n",
    "for f in features_extraction:\n",
    "    categories = np.intersect1d( df_test[f] , df_train[f])\n",
    "    #train_categories = df_train[f].unique()\n",
    "    for cat in categories:\n",
    "        df_train[f+\"_\"+str(cat)] = df_train[[f]].apply(add_binary_features, args=(cat,), axis=1)\n",
    "        df_test[f+\"_\"+str(cat)] = df_test[[f]].apply(add_binary_features, args=(cat,), axis=1)\n",
    "    \n",
    "    print \"Done with \" + str(tracker)\n",
    "    \n",
    "    tracker+=1\n",
    "    \n",
    "# TODO: drop the features\n",
    "df_train.drop(features_extraction, axis=1, inplace=True)\n",
    "df_test.drop(features_extraction, axis=1, inplace=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Sum of 0's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = [f for f in df_train.columns if f != 'TARGET' and f!='ID' ]\n",
    "#df_train['sum_of_0'] = my_features['sum_of_0']\n",
    "def calculate_zeros(cols):\n",
    "    sum_array = [] \n",
    "    for value in cols:\n",
    "        if value == 0:\n",
    "            sum_array.append(value)\n",
    "    return len(sum_array)\n",
    "\n",
    "df_train['sum_of_0'] = df_train[features].apply(calculate_zeros, axis=1)\n",
    "df_test['sum_of_0'] = df_test[features].apply(calculate_zeros, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare data for model\n",
    "y_train = df_train['TARGET'].values\n",
    "X_train = df_train.drop(['ID','TARGET'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_test = df_test['ID']\n",
    "X_test = df_test.drop(['ID'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_train = len(X_train)\n",
    "len_test = len(X_test)\n",
    "\n",
    "#scale data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# apply same transformation to test data\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(32,100,32), activation='relu', \n",
    "     \n",
    "     learning_rate_init=0.001, max_iter = 5000, random_state = 1235, \n",
    "     learning_rate='adaptive',\n",
    "                   verbose=True, batch_size=200 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.16878023\n",
      "Iteration 2, loss = 0.14171645\n",
      "Iteration 3, loss = 0.13854827\n",
      "Iteration 4, loss = 0.13694234\n",
      "Iteration 5, loss = 0.13560914\n",
      "Iteration 6, loss = 0.13525031\n",
      "Iteration 7, loss = 0.13401269\n",
      "Iteration 8, loss = 0.13341853\n",
      "Iteration 9, loss = 0.13272897\n",
      "Iteration 10, loss = 0.13212703\n",
      "Iteration 11, loss = 0.13139744\n",
      "Iteration 12, loss = 0.13121433\n",
      "Iteration 13, loss = 0.13026843\n",
      "Iteration 14, loss = 0.13015638\n",
      "Iteration 15, loss = 0.12919309\n",
      "Iteration 16, loss = 0.12856298\n",
      "Iteration 17, loss = 0.12793116\n",
      "Iteration 18, loss = 0.12781769\n",
      "Iteration 19, loss = 0.12770603\n",
      "Iteration 20, loss = 0.12653349\n",
      "Iteration 21, loss = 0.12633755\n",
      "Iteration 22, loss = 0.12550075\n",
      "Iteration 23, loss = 0.12476694\n",
      "Iteration 24, loss = 0.12462755\n",
      "Iteration 25, loss = 0.12357198\n",
      "Iteration 26, loss = 0.12331921\n",
      "Iteration 27, loss = 0.12310142\n",
      "Iteration 28, loss = 0.12195190\n",
      "Iteration 29, loss = 0.12165910\n",
      "Iteration 30, loss = 0.12090958\n",
      "Iteration 31, loss = 0.12062276\n",
      "Iteration 32, loss = 0.12009394\n",
      "Iteration 33, loss = 0.11950532\n",
      "Iteration 34, loss = 0.12027225\n",
      "Iteration 35, loss = 0.11922465\n",
      "Iteration 36, loss = 0.11793248\n",
      "Iteration 37, loss = 0.11912502\n",
      "Iteration 38, loss = 0.11763607\n",
      "Iteration 39, loss = 0.11686925\n",
      "Iteration 40, loss = 0.11634372\n",
      "Iteration 41, loss = 0.11576896\n",
      "Iteration 42, loss = 0.11586627\n",
      "Iteration 43, loss = 0.11553946\n",
      "Iteration 44, loss = 0.11459921\n",
      "Iteration 45, loss = 0.11415969\n",
      "Iteration 46, loss = 0.11367439\n",
      "Iteration 47, loss = 0.11365032\n",
      "Iteration 48, loss = 0.11268287\n",
      "Iteration 49, loss = 0.11237579\n",
      "Iteration 50, loss = 0.11186746\n",
      "Iteration 51, loss = 0.11268741\n",
      "Iteration 52, loss = 0.11429830\n",
      "Iteration 53, loss = 0.11206386\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', algorithm='adam', alpha=0.0001,\n",
       "       batch_size=200, beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
       "       epsilon=1e-08, hidden_layer_sizes=(32, 100, 32),\n",
       "       learning_rate='adaptive', learning_rate_init=0.001, max_iter=5000,\n",
       "       momentum=0.9, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1235, shuffle=True, tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC: 0.906017580468\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "y_pred= clf.predict_proba(X_test)[:,1]\n",
    "print('Overall AUC:', roc_auc_score(y_train, clf.predict_proba(X_train)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"ID\":id_test, \"TARGET\":y_pred})\n",
    "submission.to_csv(\"submission61.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nADDITIONAL FUNCTIONS FOR FEATURE ENGINEERING\\n==============================================================\\nimport pickle\\nto_save = {\\n    \"sum_of_0\":df_train[\\'sum_of_0\\']\\n}\\npickle.dump(to_save, open(\\'my_features.pickle\\',\\'a\\'))\\n===============================================================\\n#NAME: ADD SUM OF 0\\'S\\n# select on the features, leaving out the \\'TARGET\\' feature\\n\\n\\nfeatures = [f for f in df_train.columns if f != \\'TARGET\\' and f!=\\'ID\\' ]\\n\\ndef calculate_zeros(cols):\\n    sum_array = [] \\n    for value in cols:\\n        if value == 0:\\n            sum_array.append(value)\\n    return len(sum_array)\\n\\ndf_train[\\'sum_of_0\\'] = df_train[features].apply(calculate_zeros, axis=1)\\n\\n===============================================================\\n\\n# remove duplicated columns\\nremove = []\\nc = df_train.columns\\nfor i in range(len(c)-1):\\n    v = df_train[c[i]].values\\n    for j in range(i+1, len(c)):\\n        if np.array_equal(v, df_train[c[j]].values):\\n            remove.append(c[j])\\n\\ndf_train.drop(remove, axis=1, inplace=True)\\ndf_test.drop(remove, axis=1, inplace=True)\\n\\n================================================================\\n\\n# remove constant columns\\nremove = []\\nfor col in df_train.columns:\\n    if df_train[col].std() == 0:\\n        remove.append(col)\\n================================================================\\n#fix nationality\\n\\ndef fix_nationality(nat):\\n    for value in nat:\\n        if value == -999999:\\n            return 2\\n        else:\\n            return value\\n\\ndf_train[\\'var3\\'] = df_train[[\\'var3\\']].apply(fix_nationality, axis=1)\\ndf_test[\\'var3\\'] = df_test[[\\'var3\\']].apply(fix_nationality, axis=1)\\n\\n\\n=================================================================\\n# Add feature selection\\n\\n#Extract 49 features from all the features\\n\\nfrom sklearn.ensemble import ExtraTreesClassifier\\nfrom sklearn.feature_selection import SelectFromModel\\n\\nclf_0 = ExtraTreesClassifier(random_state=1729)\\nselector = clf_0.fit(X_train, y_train)\\n\\nfs = SelectFromModel(selector, prefit=True)\\n\\nX_train = fs.transform(X_train)\\nX_eval = fs.transform(X_eval)\\nX_test = fs.transform(X_test)\\n=================================================\\n\\n\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ADDITIONAL FUNCTIONS FOR FEATURE ENGINEERING\n",
    "==============================================================\n",
    "import pickle\n",
    "to_save = {\n",
    "    \"sum_of_0\":df_train['sum_of_0']\n",
    "}\n",
    "pickle.dump(to_save, open('my_features.pickle','a'))\n",
    "===============================================================\n",
    "#NAME: ADD SUM OF 0'S\n",
    "# select on the features, leaving out the 'TARGET' feature\n",
    "\n",
    "\n",
    "features = [f for f in df_train.columns if f != 'TARGET' and f!='ID' ]\n",
    "\n",
    "def calculate_zeros(cols):\n",
    "    sum_array = [] \n",
    "    for value in cols:\n",
    "        if value == 0:\n",
    "            sum_array.append(value)\n",
    "    return len(sum_array)\n",
    "\n",
    "df_train['sum_of_0'] = df_train[features].apply(calculate_zeros, axis=1)\n",
    "\n",
    "===============================================================\n",
    "\n",
    "# remove duplicated columns\n",
    "remove = []\n",
    "c = df_train.columns\n",
    "for i in range(len(c)-1):\n",
    "    v = df_train[c[i]].values\n",
    "    for j in range(i+1, len(c)):\n",
    "        if np.array_equal(v, df_train[c[j]].values):\n",
    "            remove.append(c[j])\n",
    "\n",
    "df_train.drop(remove, axis=1, inplace=True)\n",
    "df_test.drop(remove, axis=1, inplace=True)\n",
    "\n",
    "================================================================\n",
    "\n",
    "# remove constant columns\n",
    "remove = []\n",
    "for col in df_train.columns:\n",
    "    if df_train[col].std() == 0:\n",
    "        remove.append(col)\n",
    "================================================================\n",
    "#fix nationality\n",
    "\n",
    "def fix_nationality(nat):\n",
    "    for value in nat:\n",
    "        if value == -999999:\n",
    "            return 2\n",
    "        else:\n",
    "            return value\n",
    "\n",
    "df_train['var3'] = df_train[['var3']].apply(fix_nationality, axis=1)\n",
    "df_test['var3'] = df_test[['var3']].apply(fix_nationality, axis=1)\n",
    "\n",
    "\n",
    "=================================================================\n",
    "# Add feature selection\n",
    "\n",
    "#Extract 49 features from all the features\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "clf_0 = ExtraTreesClassifier(random_state=1729)\n",
    "selector = clf_0.fit(X_train, y_train)\n",
    "\n",
    "fs = SelectFromModel(selector, prefit=True)\n",
    "\n",
    "X_train = fs.transform(X_train)\n",
    "X_eval = fs.transform(X_eval)\n",
    "X_test = fs.transform(X_test)\n",
    "=================================================\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
